python train_dist_backbone.py --dataset minc_seg --aux  --model deeplab --backbone resnet50s --batch-size 16
Namespace(aux=True, aux_weight=0.2, backbone='resnet50s', base_size=520, batch_size=16, checkname='default', crop_size=480, dataset='minc_seg', dist_backend='nccl', dist_url='tcp://localhost:23456', epochs=80, eval=False, export=None, ft=False, lr=0.004, lr_scheduler='poly', model='deeplab', model_zoo=None, momentum=0.9, rank=0, rectify=False, rectify_avg=False, resume=None, se_loss=False, se_weight=0.2, seed=1, start_epoch=0, test_batch_size=16, test_folder=None, test_val=False, train_split='train', weight_decay=0.0001, workers=8, world_size=1)
rank: 0 / 1
BaseDataset: base_size 520, crop_size 480
/home/lzj/.encoding/data/minc_dataset/images/training
parameter number of no grad is
167
number of all parameter is
193
DeepLabV3(
  (pretrained): ResNet(
    (conv1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (bn1): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): DistSyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn2): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn2): DistSyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): DistSyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): GlobalAvgPool2d()
    (fc): None
  )
  (head): DeepLabV3Head(
    (aspp): ASPP_Module(
      (b0): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (b1): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
        (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (b2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
        (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (b3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
        (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (b4): AsppPooling(
        (gap): Sequential(
          (0): AdaptiveAvgPool2d(output_size=1)
          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU(inplace=True)
        )
      )
      (project): Sequential(
        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
    )
    (block): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.1, inplace=False)
      (4): Conv2d(256, 23, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): FCNHead(
    (conv5): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): DistSyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Conv2d(256, 23, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Using poly LR scheduler with warm-up epochs of 0!
Starting Epoch: 0
Total Epoches: 80

=>Epoch 0, learning rate = 0.0040,                     previous best = 0.0000
Epoch: 0, Iter: 0, Speed: 0.040 iter/sec, Train loss: 3.820
pixAcc: 0.572, mIoU1: 0.171
pixAcc: 0.646, mIoU2: 0.278
Epoch: 0, Time cost: 231.26823568344116

=>Epoch 1, learning rate = 0.0040,                     previous best = 0.4621
Epoch: 1, Iter: 0, Speed: 0.057 iter/sec, Train loss: 2.496
Epoch: 1, Time cost: 197.82845330238342

=>Epoch 2, learning rate = 0.0039,                     previous best = 0.4621
Epoch: 2, Iter: 0, Speed: 0.059 iter/sec, Train loss: 1.102
Epoch: 2, Time cost: 197.0104591846466

=>Epoch 3, learning rate = 0.0039,                     previous best = 0.4621
Epoch: 3, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.869
Epoch: 3, Time cost: 197.30168199539185

=>Epoch 4, learning rate = 0.0038,                     previous best = 0.4621
Epoch: 4, Iter: 0, Speed: 0.057 iter/sec, Train loss: 1.145
Epoch: 4, Time cost: 198.38610100746155

=>Epoch 5, learning rate = 0.0038,                     previous best = 0.4621
Epoch: 5, Iter: 0, Speed: 0.056 iter/sec, Train loss: 1.066
Epoch: 5, Time cost: 199.08066868782043

=>Epoch 6, learning rate = 0.0037,                     previous best = 0.4621
Epoch: 6, Iter: 0, Speed: 0.059 iter/sec, Train loss: 1.237
Epoch: 6, Time cost: 198.15117764472961

=>Epoch 7, learning rate = 0.0037,                     previous best = 0.4621
Epoch: 7, Iter: 0, Speed: 0.058 iter/sec, Train loss: 1.114
Epoch: 7, Time cost: 198.1323163509369

=>Epoch 8, learning rate = 0.0036,                     previous best = 0.4621
Epoch: 8, Iter: 0, Speed: 0.055 iter/sec, Train loss: 0.757
Epoch: 8, Time cost: 199.00995206832886

=>Epoch 9, learning rate = 0.0036,                     previous best = 0.4621
Epoch: 9, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.935
Epoch: 9, Time cost: 197.61041569709778

=>Epoch 10, learning rate = 0.0035,                     previous best = 0.4621
Epoch: 10, Iter: 0, Speed: 0.057 iter/sec, Train loss: 1.001
pixAcc: 0.711, mIoU1: 0.251
pixAcc: 0.746, mIoU2: 0.480
Epoch: 10, Time cost: 221.90884232521057

=>Epoch 11, learning rate = 0.0035,                     previous best = 0.6132
Epoch: 11, Iter: 0, Speed: 0.056 iter/sec, Train loss: 1.381
Epoch: 11, Time cost: 198.26400589942932

=>Epoch 12, learning rate = 0.0035,                     previous best = 0.6132
Epoch: 12, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.807
Epoch: 12, Time cost: 197.16586995124817

=>Epoch 13, learning rate = 0.0034,                     previous best = 0.6132
Epoch: 13, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.712
Epoch: 13, Time cost: 198.28742504119873

=>Epoch 14, learning rate = 0.0034,                     previous best = 0.6132
Epoch: 14, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.902
Epoch: 14, Time cost: 198.22173190116882

=>Epoch 15, learning rate = 0.0033,                     previous best = 0.6132
Epoch: 15, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.473
Epoch: 15, Time cost: 197.5891969203949

=>Epoch 16, learning rate = 0.0033,                     previous best = 0.6132
Epoch: 16, Iter: 0, Speed: 0.058 iter/sec, Train loss: 1.249
Epoch: 16, Time cost: 197.86605620384216

=>Epoch 17, learning rate = 0.0032,                     previous best = 0.6132
Epoch: 17, Iter: 0, Speed: 0.060 iter/sec, Train loss: 0.742
Epoch: 17, Time cost: 196.5820813179016

=>Epoch 18, learning rate = 0.0032,                     previous best = 0.6132
Epoch: 18, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.735
Epoch: 18, Time cost: 198.26238799095154

=>Epoch 19, learning rate = 0.0031,                     previous best = 0.6132
Epoch: 19, Iter: 0, Speed: 0.057 iter/sec, Train loss: 1.041
Epoch: 19, Time cost: 198.2557384967804

=>Epoch 20, learning rate = 0.0031,                     previous best = 0.6132
Epoch: 20, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.585
pixAcc: 0.693, mIoU1: 0.262
pixAcc: 0.761, mIoU2: 0.506
Epoch: 20, Time cost: 221.94126653671265

=>Epoch 21, learning rate = 0.0030,                     previous best = 0.6333
Epoch: 21, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.767
Epoch: 21, Time cost: 197.70399260520935

=>Epoch 22, learning rate = 0.0030,                     previous best = 0.6333
Epoch: 22, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.946
Epoch: 22, Time cost: 197.71991395950317

=>Epoch 23, learning rate = 0.0029,                     previous best = 0.6333
Epoch: 23, Iter: 0, Speed: 0.060 iter/sec, Train loss: 0.472
Epoch: 23, Time cost: 196.67423367500305

=>Epoch 24, learning rate = 0.0029,                     previous best = 0.6333
Epoch: 24, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.479
Epoch: 24, Time cost: 198.41188883781433

=>Epoch 25, learning rate = 0.0029,                     previous best = 0.6333
Epoch: 25, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.455
Epoch: 25, Time cost: 198.21416449546814

=>Epoch 26, learning rate = 0.0028,                     previous best = 0.6333
Epoch: 26, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.642
Epoch: 26, Time cost: 197.65727257728577

=>Epoch 27, learning rate = 0.0028,                     previous best = 0.6333
Epoch: 27, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.699
Epoch: 27, Time cost: 197.40410447120667

=>Epoch 28, learning rate = 0.0027,                     previous best = 0.6333
Epoch: 28, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.621
Epoch: 28, Time cost: 197.74689078330994

=>Epoch 29, learning rate = 0.0027,                     previous best = 0.6333
Epoch: 29, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.444
Epoch: 29, Time cost: 197.65109205245972

=>Epoch 30, learning rate = 0.0026,                     previous best = 0.6333
Epoch: 30, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.370
pixAcc: 0.723, mIoU1: 0.284
pixAcc: 0.767, mIoU2: 0.517
Epoch: 30, Time cost: 221.28554439544678

=>Epoch 31, learning rate = 0.0026,                     previous best = 0.6420
Epoch: 31, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.468
Epoch: 31, Time cost: 197.66006016731262

=>Epoch 32, learning rate = 0.0025,                     previous best = 0.6420
Epoch: 32, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.416
Epoch: 32, Time cost: 197.68444681167603

=>Epoch 33, learning rate = 0.0025,                     previous best = 0.6420
Epoch: 33, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.766
Epoch: 33, Time cost: 197.7152590751648

=>Epoch 34, learning rate = 0.0024,                     previous best = 0.6420
Epoch: 34, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.448
Epoch: 34, Time cost: 197.3060290813446

=>Epoch 35, learning rate = 0.0024,                     previous best = 0.6420
Epoch: 35, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.319
Epoch: 35, Time cost: 197.46441411972046

=>Epoch 36, learning rate = 0.0023,                     previous best = 0.6420
Epoch: 36, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.787
Epoch: 36, Time cost: 196.97901916503906

=>Epoch 37, learning rate = 0.0023,                     previous best = 0.6420
Epoch: 37, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.318
Epoch: 37, Time cost: 197.96141076087952

=>Epoch 38, learning rate = 0.0022,                     previous best = 0.6420
Epoch: 38, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.497
Epoch: 38, Time cost: 198.05843448638916

=>Epoch 39, learning rate = 0.0022,                     previous best = 0.6420
Epoch: 39, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.352
Epoch: 39, Time cost: 198.1325294971466

=>Epoch 40, learning rate = 0.0021,                     previous best = 0.6420
Epoch: 40, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.383
pixAcc: 0.740, mIoU1: 0.285
pixAcc: 0.773, mIoU2: 0.525
Epoch: 40, Time cost: 222.77826571464539

=>Epoch 41, learning rate = 0.0021,                     previous best = 0.6492
Epoch: 41, Iter: 0, Speed: 0.061 iter/sec, Train loss: 0.440
Epoch: 41, Time cost: 196.45212936401367

=>Epoch 42, learning rate = 0.0020,                     previous best = 0.6492
Epoch: 42, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.388
Epoch: 42, Time cost: 197.9369752407074

=>Epoch 43, learning rate = 0.0020,                     previous best = 0.6492
Epoch: 43, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.307
Epoch: 43, Time cost: 197.56380796432495

=>Epoch 44, learning rate = 0.0019,                     previous best = 0.6492
Epoch: 44, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.580
Epoch: 44, Time cost: 197.97253704071045

=>Epoch 45, learning rate = 0.0019,                     previous best = 0.6492
Epoch: 45, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.453
Epoch: 45, Time cost: 197.20460271835327

=>Epoch 46, learning rate = 0.0019,                     previous best = 0.6492
Epoch: 46, Iter: 0, Speed: 0.055 iter/sec, Train loss: 0.413
Epoch: 46, Time cost: 198.0782663822174

=>Epoch 47, learning rate = 0.0018,                     previous best = 0.6492
Epoch: 47, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.442
Epoch: 47, Time cost: 198.25543975830078

=>Epoch 48, learning rate = 0.0018,                     previous best = 0.6492
Epoch: 48, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.749
Epoch: 48, Time cost: 197.83436369895935

=>Epoch 49, learning rate = 0.0017,                     previous best = 0.6492
Epoch: 49, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.491
Epoch: 49, Time cost: 198.33684730529785

=>Epoch 50, learning rate = 0.0017,                     previous best = 0.6492
Epoch: 50, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.495
pixAcc: 0.709, mIoU1: 0.279
pixAcc: 0.773, mIoU2: 0.515
Epoch: 50, Time cost: 220.96249628067017

=>Epoch 51, learning rate = 0.0016,                     previous best = 0.6492
Epoch: 51, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.463
Epoch: 51, Time cost: 198.02313470840454

=>Epoch 52, learning rate = 0.0016,                     previous best = 0.6492
Epoch: 52, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.412
Epoch: 52, Time cost: 197.55554008483887

=>Epoch 53, learning rate = 0.0015,                     previous best = 0.6492
Epoch: 53, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.415
Epoch: 53, Time cost: 197.4544608592987

=>Epoch 54, learning rate = 0.0015,                     previous best = 0.6492
Epoch: 54, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.448
Epoch: 54, Time cost: 198.83143973350525

=>Epoch 55, learning rate = 0.0014,                     previous best = 0.6492
Epoch: 55, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.540
Epoch: 55, Time cost: 196.9916386604309

=>Epoch 56, learning rate = 0.0014,                     previous best = 0.6492
Epoch: 56, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.304
Epoch: 56, Time cost: 198.33532166481018

=>Epoch 57, learning rate = 0.0013,                     previous best = 0.6492
Epoch: 57, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.176
Epoch: 57, Time cost: 197.72580409049988

=>Epoch 58, learning rate = 0.0013,                     previous best = 0.6492
Epoch: 58, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.338
Epoch: 58, Time cost: 197.70996618270874

=>Epoch 59, learning rate = 0.0012,                     previous best = 0.6492
Epoch: 59, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.386
Epoch: 59, Time cost: 198.2139172554016

=>Epoch 60, learning rate = 0.0011,                     previous best = 0.6492
Epoch: 60, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.203
pixAcc: 0.743, mIoU1: 0.296
pixAcc: 0.781, mIoU2: 0.539
Epoch: 60, Time cost: 221.39491987228394

=>Epoch 61, learning rate = 0.0011,                     previous best = 0.6598
Epoch: 61, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.357
Epoch: 61, Time cost: 197.73242735862732

=>Epoch 62, learning rate = 0.0010,                     previous best = 0.6598
Epoch: 62, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.259
Epoch: 62, Time cost: 197.42700004577637

=>Epoch 63, learning rate = 0.0010,                     previous best = 0.6598
Epoch: 63, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.349
Epoch: 63, Time cost: 198.9230523109436

=>Epoch 64, learning rate = 0.0009,                     previous best = 0.6598
Epoch: 64, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.280
Epoch: 64, Time cost: 197.96858263015747

=>Epoch 65, learning rate = 0.0009,                     previous best = 0.6598
Epoch: 65, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.421
Epoch: 65, Time cost: 197.57219862937927

=>Epoch 66, learning rate = 0.0008,                     previous best = 0.6598
Epoch: 66, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.394
Epoch: 66, Time cost: 198.4468228816986

=>Epoch 67, learning rate = 0.0008,                     previous best = 0.6598
Epoch: 67, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.309
Epoch: 67, Time cost: 197.74797558784485

=>Epoch 68, learning rate = 0.0007,                     previous best = 0.6598
Epoch: 68, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.280
Epoch: 68, Time cost: 197.39835333824158

=>Epoch 69, learning rate = 0.0007,                     previous best = 0.6598
Epoch: 69, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.343
Epoch: 69, Time cost: 197.37386512756348

=>Epoch 70, learning rate = 0.0006,                     previous best = 0.6598
Epoch: 70, Iter: 0, Speed: 0.055 iter/sec, Train loss: 0.408
pixAcc: 0.752, mIoU1: 0.295
pixAcc: 0.786, mIoU2: 0.542
Epoch: 70, Time cost: 222.53850317001343

=>Epoch 71, learning rate = 0.0006,                     previous best = 0.6639
Epoch: 71, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.439
Epoch: 71, Time cost: 197.6670262813568

=>Epoch 72, learning rate = 0.0005,                     previous best = 0.6639
Epoch: 72, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.365
Epoch: 72, Time cost: 197.68538784980774

=>Epoch 73, learning rate = 0.0004,                     previous best = 0.6639
Epoch: 73, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.246
Epoch: 73, Time cost: 198.15702986717224

=>Epoch 74, learning rate = 0.0004,                     previous best = 0.6639
Epoch: 74, Iter: 0, Speed: 0.059 iter/sec, Train loss: 0.328
Epoch: 74, Time cost: 196.994469165802

=>Epoch 75, learning rate = 0.0003,                     previous best = 0.6639
Epoch: 75, Iter: 0, Speed: 0.057 iter/sec, Train loss: 0.348
Epoch: 75, Time cost: 198.42284750938416

=>Epoch 76, learning rate = 0.0003,                     previous best = 0.6639
Epoch: 76, Iter: 0, Speed: 0.060 iter/sec, Train loss: 0.327
Epoch: 76, Time cost: 196.83027362823486

=>Epoch 77, learning rate = 0.0002,                     previous best = 0.6639
Epoch: 77, Iter: 0, Speed: 0.054 iter/sec, Train loss: 0.275
Epoch: 77, Time cost: 198.97882986068726

=>Epoch 78, learning rate = 0.0001,                     previous best = 0.6639
Epoch: 78, Iter: 0, Speed: 0.058 iter/sec, Train loss: 0.263
Epoch: 78, Time cost: 197.9319863319397

=>Epoch 79, learning rate = 0.0001,                     previous best = 0.6639
Epoch: 79, Iter: 0, Speed: 0.056 iter/sec, Train loss: 0.270
pixAcc: 0.749, mIoU1: 0.294
pixAcc: 0.790, mIoU2: 0.545
Epoch: 79, Time cost: 221.96929001808167
pixAcc: 0.749, mIoU1: 0.294
pixAcc: 0.790, mIoU2: 0.545

